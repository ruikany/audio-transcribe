<!doctype html>
<html>
  <head>
    <title>Real-Time STT with Flask</title>
    <style>
      body {
        font-family: sans-serif;
        margin: 2em;
      }
      #transcript {
        white-space: pre-wrap;
        padding: 1em;
        background: #f4f4f4;
        border-radius: 5px;
      }
      #status {
        color: #555;
      }
    </style>
  </head>
  <body>
    <h1>Real-Time Transcription with Flask & RealtimeSTT</h1>
    <button id="recBtn">Start Recording</button>
    <p id="status">Status: Not Connected</p>
    <h3>Transcription:</h3>
    <div id="transcript"></div>

    <script>
      const recBtn = document.getElementById("recBtn");
      const transcriptEl = document.getElementById("transcript");
      const statusEl = document.getElementById("status");

      let ws;
      let audioContext;
      let audioProcessor;
      let mediaStream;

      let recording = false;

      // --- WebSocket Handling ---
      function connectWebSocket() {
        // Use wss:// for secure connections (HTTPS)
        const wsUrl = `ws://${window.location.host}/mic`;
        ws = new WebSocket(wsUrl);

        ws.onopen = () => {
          statusEl.textContent = "Status: Connected. Ready to record.";
          recBtn.disabled = false;
        };

        ws.onmessage = (event) => {
          // Server sent us a new transcription
          transcriptEl.textContent += event.data + "\n";
        };

        ws.onclose = () => {
          statusEl.textContent = "Status: Disconnected. Refresh to retry.";
          stopRecording();
        };

        ws.onerror = (err) => {
          console.error("WebSocket Error:", err);
          statusEl.textContent = "Status: WebSocket Error.";
        };
      }

      // --- Audio Processing ---
      async function startRecording() {
        if (recording) return;
        recording = true;
        recBtn.textContent = "Stop Recording";

        try {
          mediaStream = await navigator.mediaDevices.getUserMedia({
            audio: true,
          });

          // 1. THIS IS THE CHANGE:
          // Let the AudioContext use the hardware's native sample rate.
          // This will make the connection succeed.
          audioContext = new (window.AudioContext ||
            window.webkitAudioContext)();

          // Now this connection will work, as both are at the same rate
          const source = audioContext.createMediaStreamSource(mediaStream);

          audioProcessor = audioContext.createScriptProcessor(4096, 1, 1);

          // Store the native sample rate for our resampler
          const nativeSampleRate = audioContext.sampleRate;

          audioProcessor.onaudioprocess = (e) => {
            if (!ws || ws.readyState !== WebSocket.OPEN) return;

            // Get the raw audio (at native sample rate, e.g., 48000 Hz)
            const float32Data = e.inputBuffer.getChannelData(0);

            // 2. THIS IS THE NEW PART:
            // Resample the audio from native-rate down to 16000 Hz
            const resampledData = resample(
              float32Data,
              nativeSampleRate,
              16000,
            );

            // Convert to 16-bit PCM (Int16)
            const pcmData = new Int16Array(resampledData.length);
            for (let i = 0; i < resampledData.length; i++) {
              let s = Math.max(-1, Math.min(1, resampledData[i]));
              pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
            }

            // Send the 16000 Hz data
            ws.send(pcmData.buffer);
          };
          source.connect(audioProcessor);
          audioProcessor.connect(audioContext.destination); // Connect to output (necessary)
        } catch (e) {
          console.error("Error starting audio:", e);
          statusEl.textContent = "Error: Could not access microphone.";
          recording = false;
          recBtn.textContent = "Start Recording";
        }
      }

      function stopRecording() {
        if (!recording) return;
        recording = false;
        recBtn.textContent = "Start Recording";

        if (audioProcessor) {
          audioProcessor.disconnect();
          audioProcessor = null;
        }
        if (audioContext) {
          audioContext.close();
          audioContext = null;
        }
        if (mediaStream) {
          mediaStream.getTracks().forEach((track) => track.stop());
          mediaStream = null;
        }
        // Note: We don't close the WebSocket here, as the server
        // might still be processing the last bit of audio.
      }

      // --- Init ---
      recBtn.onclick = () => {
        recording ? stopRecording() : startRecording();
      };

      recBtn.disabled = true;
      connectWebSocket();
      function resample(buffer, fromSampleRate, toSampleRate) {
        if (fromSampleRate === toSampleRate) {
          return buffer;
        }

        const ratio = fromSampleRate / toSampleRate;
        const newLength = Math.floor(buffer.length / ratio);
        const result = new Float32Array(newLength);

        for (let i = 0; i < newLength; i++) {
          const floatIndex = i * ratio;
          const index = Math.floor(floatIndex);
          const fraction = floatIndex - index;

          const p0 = buffer[index];
          const p1 = buffer[index + 1];

          if (p1 === undefined) {
            // Handle end of buffer
            result[i] = p0;
          } else {
            // Simple linear interpolation
            result[i] = p0 + (p1 - p0) * fraction;
          }
        }
        return result;
      }
    </script>
  </body>
</html>
